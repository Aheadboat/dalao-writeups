<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">YOUR NAME(S)</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="TODO">https://aheadboat.github.io/dalao-writeups/proj3-1/index.html</a></h2>

<br><br>


<div align="center">
  <table style="width:100%">
      <tr>
          <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div>


<div>

<h2 align="middle">Overview</h2>
<p>
    YOUR RESPONSE GOES HERE
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
    Generating a ray requires an origin coordinate (the camera position in worldspace) and a normalized direction vector. We’re given a pixel coordinate in the camera’s space, and we have to transform it to worldspace. Given the camera’s field of view angles (hFov and vFov in degrees), we convert them to radians, and map the sensor plane to have bottom left corner at (-tan(hFov/2), -tan(vFov/2), -1) and top right corner at (tan(hFov/2), tan(vFov/2), -1). We then map (0,0) in camera space to the bottom right corner, and (1, 1) in camera space to the top right corner, and multiply the pixel coordinate with the matrix c2w, converting it to world space. 
	Finding intersections between rays and scene objects required using vectors to represent the primitives themselves. Given that a ray is parameterized by r(t) = O + tD, 0 <= t <= infinity, we can find the intersection between a ray and a primitive using the Moller-Trumbore algorithm as pictured: 
</p>
<img src="images/moller-trumbore.jpg"/>
<p>
After performing the test, we will check to see if the t-values we’ve retrieved are valid. If t < min_t or t > max_t, we know the t is invalid and thus return false. Else, we update the t value so future intersection tests will not return true. 

</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
    The triangle intersection algorithm I implemented used the Moller-Trumbore algorithm to find the barycentric coordinates and the t-value of the ray-intersection with the triangle. If the barycentric coordinates were invalid (i.e. b > 1 or b < 0), or if the t value was invalid, then we knew the ray didn’t intersect the triangle. 

</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Task1/CBspheres.png.png" align="middle" width="400px"/>
        <figcaption>CBSpheres.dae</figcaption>
      </td>
      <td>
        <img src="images/Task1/cow.png" align="middle" width="400px"/>
        <figcaption>cow.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
    I used the SAH heuristic. For each axis (x, y, z), I would go through and iterate over every primitive's centroid, calculate the SAH based on splitting primitives at that centroid, and split based on the best. I'd choose the lowest cost centroid/axis, and split there. The issue with this heuristic is that it grows by n^2, so its actually decently slow for large, large files.
SAH: (# of objects split into left) * (surface area of left bbox) + (# of objects split into right) * (surface area of right bbox)

</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Task2/CBLucy.png" align="middle" width="400px"/>
        <figcaption>CBLucy.dae</figcaption>
      </td>
      <td>
        <img src="images/Task2/MaxPlanck.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
    For CBLucy.dae, before BVH acceleration we get 35146.32 intersection tests per ray, whereas after BVH acceleration we get around 6 per ray. With a BVH, the number of intersections per ray becomes O(logn) whreeas without BVH it is O(n), meaning the mesh size can be increased without significantly affecting computation time. 
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
    Two implementations of direct lighting we explored in this project were Uniform Hemisphere sampling and Importance sampling.
Uniform Hemisphere Sampling:
We calculate the local coordinate space such that the normal is (0,0,1) using the input intersection normal. Then, for num_sample times, we sample a random unit vector wi from the hemisphere using hemisphereSampler.get_sample(), then transform that unit vector to worldspace using o2w transformation matrix. Then, we create a new Intersection object (to track the emission/reflectance of the surface we intersect with), add a small offset to the ray’s min_t (EPS_F) as well as the cosine of wi. Lastly, we divide by the pdf (which is always 1/2pi) and return the average light by dividing by num_samples. 
Importance Sampling:
Importance sampling samples the lights directly, speeding up the convergence since in uniform hemisphere sampling, a lot of rays don’t intersect with lights. 
For each light in the scene:
If the light is a delta light (point light), we only need to sample once. 
Otherwise, we will sample ns_area_light times. 
We call sample_L, which returns the radiance and which gives us a sampled direction from the hit_point to the light source, the pdf of that sample, and the distance to light.
We create a new shadow ray originating from the hit-point with direction wi, with max_t equal to the distToLight. If intersect(shadowRay, &new_isect) returns true, then we know that there is an obstruction and return (0,0,0). Else, we know there is no obstruction and we can use the same formula (f (radiance from sample_L) * isect.bsdf->get_emission() (amount of light absorbed by surface) * cosTheta (cosine of vector wi in object space) / pdf. 
We average light by number of samples taken (ns_area_light). 

</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/Task3/CBbunny_hemisphere.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae with UHS</figcaption>
      </td>
      <td>
        <img src="images/Task3/CBbunny_importance.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae with Light Sampling</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/Task3/CBspheres_hemisphere.png" align="middle" width="400px"/>
        <figcaption>CBSpheres_lambertian.dae with UHS</figcaption>
      </td>
      <td>
        <img src="images/Task3/CBspheres_importance.png" align="middle" width="400px"/>
        <figcaption>CBSpheres_lambertian.dae with Light Sampling</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Task3/CBspheres_1_1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (CBSpheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task3/CBspheres_1_4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (CBSpheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Task3/CBspheres_1_16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (CBSpheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task3/CBspheres_1_64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (CBSpheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    As we increase the number of light rays sampled, the noise decreases linearly. This is because the variance in lighting decreases as we increase the sample size. 
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
    Lighting sampling has much smoother renderings than uniform hemisphere sampling because it does not waste time on rays that miss light sources. Uniform hemisphere sampling has much more noise as well due to this fact.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
   I had to go back and change raytrace pixel to initialize the depth to max_ray_depth. In the indirect lighting function (at_least_one_bounce), I first check to see if we’ve reached maximum depth (r.depth < 1). If so, I will immediately return the L_out value, which will be the value of calling the one_bounce function. Else, I will flip the coin for Russian Roulette. If it fails (terminates), then we return L_out. Else, we will recursively call another bounce by calculating the bounce ray using hit_p and a direction vector (which we retrieve from sample_f and then transform into worldspace using o2w), and set the depth to be the current ray’s depth + 1. Then we recursively calculate the Luminance using another at_least_one_bounce call, using the same equation as in Task 3 but dividing by the probability. 

</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Task4/beast_global_illum.png" align="middle" width="400px"/>
        <figcaption>beast.dae</figcaption>
      </td>
      <td>
        <img src="images/Task4/bunny_global_illum.png" align="middle" width="400px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Task4/CBbunny_direct_illum.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task4/CBbunny_indirect_illum.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    The indirect illumination is only lit by indirect bounces of light. Thus, parts of the scene that aren't hit by the direct lighting, like the undersides of the bunny, are lit. 
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Task4/CBbunny_m0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task4/CBbunny_m1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Task4/CBbunny_m2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task4/CBbunny_m3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Task4/CBbunny_m100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    The most visible change is from m=0 to m=1. After that, the changes are much smaller as we weigh them much less. As observed, there is very very little difference between m=3 and m=100. 
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Task4/CBspheres_1spp.png.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task4/CBspheres_2spp.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Task4/CBspheres_4spp.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task4/CBspheres_8spp.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Task4/CBspheres_16spp.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task4/CBspheres_64spp.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Task4/CBspheres_1024spp.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Global illumination requires a large spp (sample-per-pixel) rate to converge to a noise-less rendering (i.e. it is very expensive). 
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    Adaptive sampling aims to shorten the sampling process by stopping sampling once the pixel reaches a reasonable threshold of convergence (satisfactory noise levels). My implementation of the adaptive sampling algorithm uses the same sampling method as in raytrace_pixel(), but keeps track of two values: the summation of each sample’s illuminance x_k, and the summation of each sample’s illuminance squared x_k^2. The mean and variance of the sample are given by: </p>
<p> We use these values to calculate I, our threshold. If I < maxTolerance * mean, then we can stop sampling. We check I every samplesPerBatch number of samples, making sure to then average the illuminance by the total number of samples taken for the final value. 

</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Task5/CBbunny_adaptive.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task5/CBbunny_adaptive_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Task5/CBSpheres_adaptive.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBSpheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/Task5/CBSpheres_adaptive_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
